{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to investigate the methods of getting VJPs in different AD packages - namely Autograd, TensorFlow, and PyTorch, and clarify the behaviors of the relevant functions implemented in these packages. In cases where we are only interested in the gradient of the scalar loss function with regards to parameters, it is not necessary to explicitly build the intermediate VJPs as we can always get the VJP at any nodes by asking the automatic differentiator to calculate the gradient of the loss function with regards to those particular parameters. However, it can become essential if we want to calculate the product between a vector-vector Jacobian and an arbitrary vector which is not the derivative of any scalar node in the graph (that is not something like `dL / dv`) - say, in Gauss-Newton optimization, where we need to calculate the product between a Jacobian and a step vector. Moreover, writing the backpropagation process as a chain of VJPs provides insight for one to understand how things really work. We will assume readers have basic knowledge on the mathematical definitions of gradient, Jacobian matrix, Hessian matrix, and preferably, on the basic principle of backpropagation. \n",
    "\n",
    "The Gauss-Newton optimization codes by Saugat Kandel (https://github.com/saugatkandel/sopt) were heavily used as a reference when I tried to figure out how the `make_vjp` function in Autograd works. If you are looking for a working implementation of second-order optimization algorithms, please check out the cited repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a simple model of `h(x) = y2(y1(x)) = exp(x^2)`, that is, `y2(g) = exp(g)` and `y1(g) = g^2`. The loss function `L = ||h(x) - y||^2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider an input vector `x = [1, 2]`. The prediction function should give an output vector of `h = [2.71828183, 54.59815003]`. Suppose `y = [9.48773584, 518.01282467]` (which is the output that would be given by `x = [1.5, 2.5]`), the value of the scalar loss function should be `L = 214798.98617481123`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytically, the Jacobian of `L` with regards to `h` for a 2-D vector input is\n",
    "```\n",
    "J_L_h = [2 * (h1 - y1), 2 * (h2 - y2)] = [-13.53890802, -926.82934927]\n",
    "```\n",
    "(note that `h` is also `f2`). Since `L` is a scalar, the Jacobian of `L` with regards to `h` is a vector. Although not entirely proper to write in this way, we can slightly abuse the notation and say\n",
    "```\n",
    "J_L_h ?= dL / dh = dL / df2.\n",
    "```\n",
    "The Jacobian of `h` (or `f2`) with reagrds to `f1` is\n",
    "```\n",
    "J_f2_f1 = [[exp(x1^2), 0        ],\n",
    "            0        , exp(x2^2)]]\n",
    "       ?= df2 / df1.\n",
    "```\n",
    "The VJP of `L` at `f1`, is then\n",
    "```\n",
    "VJP_L_f1 = J_f2_f1 * J_L_h = [-3.68025676e+01, -5.06031679e+04].\n",
    "```\n",
    "Again with the abuse of notation, we can recognize the nature of `VJP_f1` more clearly as\n",
    "```\n",
    "VJP_L_f1 = dL / df1.\n",
    "```\n",
    "In many literatures, it is more commonly written as `f1` with a dot on the top.\n",
    "\n",
    "Similarly, the Jacobian of `f1` with regards to `x` is\n",
    "```\n",
    "J_f1_x = [[2 * x_1, 0       ],\n",
    "          [0      , 2 * x_2 ]]\n",
    "      ?= df1 / dx. \n",
    "```\n",
    "and the VJP at `x` is then\n",
    "```\n",
    "VJP_L_x = J_f1_x * VJP_x\n",
    "        = [-7.36051353e+01, -2.02412671e+05]\n",
    "       ?= dL / dx.\n",
    "```\n",
    "That is, `VJP_x` gives the gradient of `L` with regards to the input vector `x`.\n",
    "\n",
    "Lastly, we may also be interested in the Hessian of `L` with regards to `h`, which is crucial for implementing Gauss-Newton-based second-order optimization algorithms. The closed-form expression is rather simple for a least-square type of loss function:\n",
    "```\n",
    "d^2L / dh^2 = diag(2)\n",
    "```\n",
    "so the Hessian-vector-product (HVP) of any vector `v` with it would be `2v`. \n",
    "\n",
    "To summarize, numerically we have these VJPs:\n",
    "```\n",
    "dL / dh = [-13.53890802, -926.82934927]\n",
    "dL / df1 = [-3.68025676e+01, -5.06031679e+04]\n",
    "dL / dx = [-7.36051353e+01, -2.02412671e+05]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create this model in Autograd first, and use the `make_vjp` function to get the intermediate VJPs with cross-validation with the above values that we have derived by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-order operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define all the intermediate functions, and the final loss below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "f1 = lambda g: g ** 2\n",
    "f2 = lambda g: np.exp(g)\n",
    "h = lambda x: f2(f1(x))\n",
    "L = lambda h, y: np.sum((h - y) ** 2) \n",
    "L_x = lambda x, y: np.sum((h(x) - y) ** 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have created two seemingly redundant instances of loss function: one takes the prediction functiuon `h` as input, and the other takes `x`. This difference matters because the input node determines \"with regards to whom the VJP should be calculated\". Now if we feed in the values of `x` and `y`, we get the supposed output values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is  214798.98617639724\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1., 2.])\n",
    "y = np.array([9.48773584, 518.01282467])\n",
    "print('Loss is ', L_x(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grad` function of Autograd takes in the loss function, and return a function that has the same arguments as the loss, but returns the gradient instead. The second argument of `grad` is used to specify the variable that the gradient should be calculated for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient dL / dx is  [-7.36051353e+01 -2.02412671e+05]\n",
      "Gradient dL / dh is  [ -13.53890802 -926.82934927]\n"
     ]
    }
   ],
   "source": [
    "grad_L_x_func = grad(L_x, [0])\n",
    "J_L_x = grad_L_x_func(x, y)\n",
    "print('Gradient dL / dx is ', dLdx[0])\n",
    "\n",
    "grad_L_h_func = grad(L, [0])\n",
    "J_L_h = grad_L_h_func(h(x), y)\n",
    "print('Gradient dL / dh is ', j_L_h[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These agree with our hand-derived solutions! But sometimes we also want to know the intermediate Jacobians and VJPs - in particular, the Jacobian of `h` at `x` (which gives `dh / dx`), which is essential when using a Gauss-Newton type of second-order optimization method. In this case we don't need to explicitly know the matrix form of Jacobian `dh / dx`, but it's crucial for us to know the VJP of `dh / dx` with any input vector `v`. The way to get the VJPs in autograd is to use the `make_vjp` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import make_vjp\n",
    "vjp_h_builder = make_vjp(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_vjp(h)` function returns a builder function `vjp_L_builder(x)`. `vjp_L_builder(x)` takes in the same argument as `h`, and gives a tuple `(vjp_h_x_func(v), h)`. `vjp_h_x_func(v)` is yet another function, which takes in the \"vector\" of the vector-Jacobian product of `L` at `h` (i.e., `dL / dh`), and returns the VJP at `v`. \n",
    "\n",
    "**Another way to understand it is to think that `vjp_h_x_func` itself is the Jacobian matrix that gives `dh / dx`, and the input `v` is `dL / dh`, so that the output will be `dL / dv`.**\n",
    "\n",
    "The ASCII diagram below illustrates the flow:\n",
    "```\n",
    "make_vjp(h) -> vjp_h(x) -> vjp_h_x(v) -> dL/dv\n",
    "                              |    \\_  \n",
    "                              |      |\n",
    "                            dh/dx * dL/dh (or any vector input)\n",
    "                          Jacobian  Vector\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vjp_h_x_func, h_value = vjp_h_builder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, `vjp_h_x_func` is equivalent to Jacobian `dh / dx` (though we don't know, and don't have to know the full matrix of the Jacobian); if we pass the value of `dL / dh` to it, we should end up with the value of `dL / dx`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL / dx is  [-7.36051353e+01 -2.02412671e+05]\n"
     ]
    }
   ],
   "source": [
    "print('dL / dx is ', vjp_h_x_func(J_L_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if we want to get `dL / df1` (whose value should be [-3.68025676e+01, -5.06031679e+04])? We need to get calculate the VJP of `dh / f1 * dL / dh`. For this, we need to construct another instance of `h` that takes `f1` as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_f1 = lambda f1: f2(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then get the VJP builder function following the similar procedure above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vjp_h_f1_builder = make_vjp(h_f1)\n",
    "f1_value = f1(x)\n",
    "vjp_h_f1_func, h_value = vjp_h_f1_builder(f1_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vjp_h_f1_func` is essentially `dh / df1`. To get `dL / df1`, we pass to it `dL / dh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL / df1 is  [[-3.68025677e+01 -5.06031679e+04]]\n"
     ]
    }
   ],
   "source": [
    "print('dL / df1 is ', vjp_h_f1_func(J_L_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, to get the VJP function `df1 / dx`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vjp_f1_builder = make_vjp(f1)\n",
    "vjp_f1_x_func, f1_value = vjp_f1_builder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these, we can then formulate the backpropagation process, which should give the ultimate gradient `dL / dx` (supposed to be [-7.36051353e+01 -2.02412671e+05]) by nesting all these VJP functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ultimate gradient is  [-7.36051353e+01 -2.02412671e+05]\n"
     ]
    }
   ],
   "source": [
    "J_L_x_explicit = vjp_f1_x_func(vjp_h_f1_func(J_L_h))\n",
    "print('The ultimate gradient is ', J_L_x_explicit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can get the same result using the `grad()` function, but building all the VJPs gives us an insight of how backpropagation functions behind the scene. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd provides a specific API for calculating the HVP through its `make_hvp` function, which behaves similarly to the `make_vjp` function. In fact, the `make_hvp(fun)` function is implemented simply as a callable that returns\n",
    "```\n",
    "make_vjp(grad(fun), x).\n",
    "```\n",
    "As noted previously, the Hessian of `L` with regards to `h` is a diagonal matrix `diag(2)`. Let us verify if the HVP of it with any vector `v` is `2v`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HVP is [1, 2] is  [2. 4.]\n"
     ]
    }
   ],
   "source": [
    "from autograd.differential_operators import make_hvp\n",
    "hvp_L_builder = make_hvp(L) # Note: L = L(h).\n",
    "hvp_L_h_func, _ = hvp_L_builder(h(x), y)\n",
    "print('HVP is [1, 2] is ', hvp_L_h_func([1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow's eager execution version has a `make_vjp` function which shares basically the same behavior with the one in Autograd, but if you don't use eager execution (which has poorer performance for large models), you need to build the VJP using `tf.gradient`. The good thing is that the `tf.gradient` function is able to give the gradient of `L` with regards to any node in the graph, so that we don't have to worry about what exactly is the input of the function we are differentiating. First, build the model using Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant([1., 2.], name='x')\n",
    "y = tf.constant([9.48773584, 518.01282467], name='y')\n",
    "f1 = tf.pow(x, 2, name='f1')\n",
    "h = tf.exp(f1, name='h')\n",
    "L = tf.reduce_sum(tf.squared_difference(h, y), name='L') \n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we attempt to reproduce all the VJPs we found above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values of dL/dh, dL/df1, and dL/dx are respectively:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[array([ -13.538908, -926.82935 ], dtype=float32)],\n",
       " [array([-3.6802567e+01, -5.0603168e+04], dtype=float32)],\n",
       " [array([-7.3605133e+01, -2.0241267e+05], dtype=float32)]]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The values of dL/dh, dL/df1, and dL/dx are respectively:')\n",
    "sess.run([tf.gradients(L, h),\n",
    "          tf.gradients(L, f1),\n",
    "          tf.gradients(L, x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask TensorFlow to calculate the gradient of `h` with regards to `x`, which should be a Jacobian matrix since both `h` and `x` are vectors, it will in fact output the VJP of the Jacobian with an all-1 vector by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of dh/dx is  [array([  5.4365635, 218.3926   ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print('The value of dh/dx is ', sess.run(tf.gradients(h, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we want to calculate the VJP with an arbitrary vector (i.e., how can we get a function for the VJP that can take in an arbitrary vector as input, as what we did with Autograd)? For example, the Jacobian `dh / df1` is\n",
    "```\n",
    "J_f2_f1 = [[exp(x1^2), 0        ],  = [[2.71828183, 0           ],\n",
    "           [0        , exp(x2^2)]]     [0         , 54.59815003 ]].\n",
    "```\n",
    "If we explicitly use the Jacobian to calculate its product with `J_L_h = [-13.53890802, -926.82934927]`, we should end up with [-3.68025676e+01, -5.06031679e+04]. This should be done by passing the vector `v` to the `grad_ys` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VJP dh/df1 * dL/dh is [array([-3.6802567e+01, -5.0603168e+04], dtype=float32)]\n",
      "VJP dh/df1 * [1, 2] is [array([  2.7182817, 109.1963   ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "J_L_h = tf.constant([-13.53890802, -926.82934927])\n",
    "vjp_h_f1_func = lambda v: tf.gradients(h, f1, grad_ys=[v])\n",
    "print('VJP dh/df1 * dL/dh is', sess.run(vjp_h_f1_func(J_L_h)))\n",
    "print('VJP dh/df1 * [1, 2] is', sess.run(vjp_h_f1_func([1., 2.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the HVP, we can pass the gradient tensor to another `tf.gradients` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HVP is [1, 2] is  [array([2., 4.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "J_L_h_tensor = tf.gradients(L, h)\n",
    "hvp_L_h_func = lambda v: tf.gradients(J_L_h_tensor, h, grad_ys=v)\n",
    "print('HVP is [1, 2] is ', sess.run(hvp_L_h_func(tf.constant([1., 2.]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
