{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to investigate the methods of getting VJPs in different AD packages - namely Autograd and PyTorch, and clarify the behaviors of the relevant functions implemented in these packages.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a simple model of `h(x) = y2(y1(x)) = exp(x^2)`, that is, `y2(g) = exp(g)` and `y1(g) = g^2`. The loss function `L = ||h(x) - y||^2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider an input vector `x = [1, 2]`. The prediction function should give an output vector of `h = [2.71828183, 54.59815003]`. Suppose `y = [9.48773584, 518.01282467]` (which is the output that would be given by `x = [1.5, 2.5]`), the value of the scalar loss function should be `L = 214798.98617481123`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytically, the Jacobian of `L` with regards to `h` for a 2-D vector input is\n",
    "```\n",
    "J_L_h = [2 * (h1 - y1), 2 * (h2 - y2)] = [-13.53890802, -926.82934927]\n",
    "```\n",
    "(note that `h` is also `f2`). Since `L` is a scalar, the Jacobian of `L` with regards to `h` is a vector. Although not entirely proper to write in this way, we can slightly abuse the notation and say\n",
    "```\n",
    "J_L_h ?= dL / dh = dL / df2.\n",
    "```\n",
    "The Jacobian of `h` (or `f2`) with reagrds to `f1` is\n",
    "```\n",
    "J_f2_f1 = [[exp(x1^2), 0        ],\n",
    "            0        , exp(x2^2)]]\n",
    "       ?= df2 / df1.\n",
    "```\n",
    "The VJP of `L` at `f1`, is then\n",
    "```\n",
    "VJP_L_f1 = J_f2_f1 * J_L_h = [-3.68025676e+01, -5.06031679e+04].\n",
    "```\n",
    "Again with the abuse of notation, we can recognize the nature of `VJP_f1` more clearly as\n",
    "```\n",
    "VJP_L_f1 = dL / df1.\n",
    "```\n",
    "In many literatures, it is more commonly written as `f1` with a dot on the top.\n",
    "\n",
    "Similarly, the Jacobian of `f1` with regards to `x` is\n",
    "```\n",
    "J_f1_x = [[2 * x_1, 0       ],\n",
    "          [0      , 2 * x_2 ]]\n",
    "      ?= df1 / dx. \n",
    "```\n",
    "and the VJP at `x` is then\n",
    "```\n",
    "VJP_L_x = J_f1_x * VJP_x\n",
    "        = [-7.36051353e+01, -2.02412671e+05]\n",
    "       ?= dL / dx.\n",
    "```\n",
    "That is, `VJP_x` gives the gradient of `L` with regards to the input vector `x`.\n",
    "\n",
    "If, instead, the \"terminal node\" of the VJPs is `h`, i.e., when we say \"VJP at `g`\", we are referring to something like `dh / dg`, then we can use the same strategy to find the VJP of `h` at `x` as\n",
    "```\n",
    "VJP_h_x = [5.43656366, 218.39260013].\n",
    "```\n",
    "\n",
    "Lastly, we may also be interested in the Hessian of `L` with regards to `h`, which is crucial for implementing Gauss-Newton-based second-order optimization algorithms. The closed-form expression is rather simple for a least-square type of loss function:\n",
    "```\n",
    "d^2L / dh^2 = diag(2)\n",
    "```\n",
    "so the Hessian-vector-product (HVP) of any vector `v` with it would be `2v`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create this model in Autograd first, and use the `make_vjp` function to get the intermediate VJPs with cross-validation with the above values that we have derived by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-order operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define all the intermediate functions, and the final loss below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "f1 = lambda g: g ** 2\n",
    "f2 = lambda g: np.exp(g)\n",
    "h = lambda x: f2(f1(x))\n",
    "L = lambda h, y: np.sum((h - y) ** 2) \n",
    "L_x = lambda x, y: np.sum((h(x) - y) ** 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have created two seemingly redundant instances of loss function: one takes the prediction functiuon `h` as input, and the other takes `x`. This difference matters because the input node determines \"with regards to whom the VJP should be calculated\". Now if we feed in the values of `x` and `y`, we get the supposed output values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is  214798.98617639724\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1., 2.])\n",
    "y = np.array([9.48773584, 518.01282467])\n",
    "print('Loss is ', L_x(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grad` function of Autograd takes in the loss function, and return a function that has the same arguments as the loss, but returns the gradient instead. The second argument of `grad` is used to specify the variable that the gradient should be calculated for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient dL / dx is  [-7.36051353e+01 -2.02412671e+05]\n",
      "Gradient dL / dh is  [ -13.53890802 -926.82934927]\n"
     ]
    }
   ],
   "source": [
    "grad_L_x_func = grad(L_x, [0])\n",
    "J_L_x = grad_L_x_func(x, y)\n",
    "print('Gradient dL / dx is ', dLdx[0])\n",
    "\n",
    "grad_L_h_func = grad(L, [0])\n",
    "J_L_h = grad_L_h_func(h(x), y)\n",
    "print('Gradient dL / dh is ', j_L_h[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These agree with our hand-derived solutions! But sometimes we also want to know the intermediate Jacobians and VJPs - in particular, the Jacobian of `h` at `x` (which gives `dh / dx`), which is essential when using a Gauss-Newton type of second-order optimization method. In this case we don't need to explicitly know the matrix form of Jacobian `dh / dx`, but it's crucial for us to know the VJP of `dh / dx` with any input vector `v`. The way to get the VJPs in autograd is to use the `make_vjp` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import make_vjp\n",
    "vjp_h_builder = make_vjp(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_vjp(h)` function returns a builder function `vjp_L_builder(x)`. `vjp_L_builder(x)` takes in the same argument as `h`, and gives a tuple `(vjp_h_x_func(v), h)`. `vjp_h_x_func(v)` is yet another function, which takes in the \"vector\" of the vector-Jacobian product of `L` at `h` (i.e., `dL / dh`), and returns the VJP at `v`. \n",
    "\n",
    "**Another way to understand it is to think that `vjp_h_x_func` is the Jacobian matrix that gives `dh / dx`, and the input `v` is `dL / dh`, so that the output will be `dL / dv`.**\n",
    "\n",
    "The ASCII diagram below illustrates the flow:\n",
    "```\n",
    "make_vjp(h) -> vjp_h(x) -> vjp_h_x(v) -> dL/dv\n",
    "                              |    \\_  \n",
    "                              |      |\n",
    "                            dh/dx * dL/dh\n",
    "                          Jacobian  Vector\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vjp_h_x_func, h_value = vjp_h_builder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, `vjp_h_x_func` is equivalent to Jacobian `dh / dx` (though we don't know, and don't have to know the full matrix of the Jacobian); if we pass the value of `dL / dh` to it, we should end up with the value of `dL / dx`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL / dx is  [-7.36051353e+01 -2.02412671e+05]\n"
     ]
    }
   ],
   "source": [
    "print('dL / dx is ', vjp_h_x_func(J_L_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if we want to get `dL / df1` (whose value should be [-3.68025676e+01, -5.06031679e+04])? We need to get calculate the VJP of `dh / f1 * dL / dh`. For this, we need to construct another instance of `h` that takes `f1` as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_f1 = lambda f1: f2(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then get the VJP builder function following the similar procedure above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vjp_h_f1_builder = make_vjp(h_f1)\n",
    "f1_value = f1(x)\n",
    "vjp_h_f1_func, h_value = vjp_h_f1_builder(f1_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vjp_h_f1_func` is essentially `dh / df1`. To get `dL / df1`, we pass to it `dL / dh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL / df1 is  [[-3.68025677e+01 -5.06031679e+04]]\n"
     ]
    }
   ],
   "source": [
    "print('dL / df1 is ', vjp_h_f1_func(J_L_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
